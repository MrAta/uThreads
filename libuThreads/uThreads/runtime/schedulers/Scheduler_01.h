/*******************************************************************************
 *     Copyright Â© 2015, 2016 Saman Barghi
 *
 *     This program is free software: you can redistribute it and/or modify
 *     it under the terms of the GNU General Public License as published by
 *     the Free Software Foundation, either version 3 of the License, or
 *     (at your option) any later version.
 *
 *     This program is distributed in the hope that it will be useful,
 *     but WITHOUT ANY WARRANTY; without even the implied warranty of
 *     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *     GNU General Public License for more details.
 *
 *     You should have received a copy of the GNU General Public License
 *     along with this program.  If not, see <http://www.gnu.org/licenses/>.
 *******************************************************************************/

#ifndef SRC_RUNTIME_SCHEDULERS_SCHEDULER_01_H_
#define SRC_RUNTIME_SCHEDULERS_SCHEDULER_01_H_

#include "../../generic/IntrusiveContainers.h"
#include "../Cluster.h"
#include "../kThread.h"
#include "../../io/IOHandler.h"

namespace uThreads {
namespace runtime {
using uThreads::io::IOHandler;
/*
 * Per uThread variable used by scheduler
 */
struct UTVar {
};
/*
 * Local kThread objects related to the
 * scheduler. will be instantiated by static __thread
 */
struct KTLocal {
    /*
     * local readyQueue for kThread which is only
     * accessible by this kThread (thus thread local).
     * This is used to bulk pull uThreads from the central
     * ReadyQueue. The bulk operation lowers the overhead
     * of pulling threads and accessing the central ReadyQueue
     * as the central ReadyQueue i protected by a mutex.
     */
    IntrusiveQueue <uThread> lrq;
};

/*
 * Per kThread variable related to the scheduler
 */
struct KTVar {
    /*
     *  Condition variable to be used by Cluster's
     *  ReadyQueue. Each kThread provides its own CV
     *  in order to provide a LIFO blocking order.
     */
    std::condition_variable cv;
    /*
     * cv_flag is used to detect spurious
     * wake ups.
     */
    bool cv_flag;

    KTVar() : cv_flag(false) {}
};

/*
 * Cluster variables
 */
struct ClusterVar {
    /*
     * This list is used to schedule uThreads in bulk.
     * For now it is only used in IOHandler
     */
    IntrusiveQueue <uThread> bulkQueue;

    /*
     * Count the number of items in bulkQueue
     */
    size_t bulkCounter;
};

class Scheduler {
    friend class kThread;

    friend class Cluster;

    friend class IOHandler;

    friend class uThread;

 private:
    /*
     * Start and end points for the exponential spin
     */
    const uint32_t SPIN_START = 4;
    const uint32_t SPIN_END = 4 * 1024;

    std::mutex mtx;  //  mutex to protect the queue
    volatile unsigned int size;  //  keep track of the size of the queue
    /*
     * The main producer-consumer queue
     * to keep track of uThreads in the Cluster
     */
    IntrusiveQueue <uThread> queue;
    /*
     * The goal is to use a LIFO ordering
     * for kThreads, so the queue can self-adjust
     * itself based on the workload. Thus, a stack
     * is used to keep track of kThreads blocked on
     * an empty queue.
     */
    IntrusiveList <kThread> ktStack;
    /*
     * This variable is used to keep
     * a history of the latest number
     * of uThreads popped from the queue.
     * This will be used to calculate the
     * new number.
     */
    size_t lastPopNum;

    Scheduler() : size(0), lastPopNum(0) {}

    virtual ~Scheduler() {}

    /*
     * Remove multiple uThreads from the queue
     * all at once. 'popnum' is calcluated
     * everytime based on the last popnum, size
     * of the queue and number of kThreads in
     * the cluster.
     * All uThreads are directly moved to the
     * local queue of the kThread (nqueue) passed
     * to the function in bulk. This saves a lot
     * of overhead.
     */
    ssize_t __removeMany(IntrusiveQueue <uThread> &nqueue) {
        // TODO(saman): is 1 (fall back to one task per each call)
        //  is a good number or should we used size%numkt
        // To avoid emptying the queue and not leaving enough work for
        // other kThreads only move a portion of the queue
        size_t numkt = kThread::currentKT->localCluster->getNumberOfkThreads();
        size_t popnum = (size / numkt) ? (size / numkt) : 1;
        popnum = (popnum < lastPopNum) ? lastPopNum : popnum;
        popnum = (popnum > size) ? size : popnum;
        lastPopNum = popnum;

        uThread *ut;
        ut = queue.front();
        nqueue.transferFrom(queue, popnum);
        size -= popnum;
        return popnum;
    }

    /*
     * Unblock a kThread that is waiting for
     * uThreads to arrive. Each kThread has its
     * own Condition Variable, and since kThreads
     * are lining up in LIFO order in ktStack, thus
     * this code pops a kThread and unblock it through
     * sending a notificiation on its CV.
     */
    inline void __unBlock() {
        if (!ktStack.empty()) {
            kThread *kt = ktStack.back();
            ktStack.pop_back();
            kt->ktvar->cv_flag = true;
            kt->ktvar->cv.notify_one();
        }
    }

    /*
     * This function uses std::mutex to exponentially spin
     * to provide a semi-spinlock behavior. If after the
     * first round of exponential spin the lock is not still
     * available, simply block. This is to avoid overhead of
     * blocking, although it adds some overhead to cache misses
     * happening by accessing the mutex memory address.
     * It can also cause out of ordeing accesses to the mutex (not that
     * it was fully FIFO before), which is somehow desired to provide
     * a chance for producers not to be preceded by many consumers that
     * just need to check whether the queue is empty and block on that.
     */
    inline void __spinLock(std::unique_lock<std::mutex> &mlock) {
        size_t spin = SPIN_START;
        for (;;) {
            if (mlock.try_lock()) break;
            // exponential spin
            for (int i = 0; i < spin; i += 1) {
                asm volatile("pause");
            }
            spin += spin;                   // powers of 2
            if (spin > SPIN_END) {
                // spin = SPIN_START;       // prevent overflow
                mlock.lock();
                break;
            }
        }
    }

    /*
     * Try popping one item and do not block.
     * give up immediately if cannot acquire
     * the mutex or the queue is empty.
     */

    uThread *__tryPop() {              // Try to pop one item, or return null
        uThread *ut = nullptr;
        /*
         * Do not block on the lock,
         * return immediately.
         */
        std::unique_lock<std::mutex> mlock(mtx, std::try_to_lock);
        // no uThreads, or could not acquire the lock
        if (mlock.owns_lock() && size != 0) {
            ut = queue.front();
            queue.pop();
            size--;
        }
        return ut;
    }

    /*
     * Try popping multiple items and do not block.
     * give up immediately if cannot acquire
     * the mutex or the queue is empty.
     */
    ssize_t __tryPopMany(IntrusiveQueue <uThread> &nqueue) {
        std::unique_lock<std::mutex> mlock(mtx, std::try_to_lock);
        if (!mlock.owns_lock()) return -1;
        if (size == 0) return 0;
        return __removeMany(nqueue);
    }

    /*
     * Pop multiple items from the queue and block
     * if the queue is empty.
     */
    ssize_t __popMany(IntrusiveQueue <uThread> &nqueue) {
        std::unique_lock<std::mutex> mlock(mtx, std::defer_lock);
        __spinLock(mlock);
        if (fastpath(size == 0)) {
            // Push the kThread to stack before waiting on it's cv
            ktStack.push_back(*kThread::currentKT);
            /*
             * Set the cv_flag so we can identify
             * spurious wake ups from notifies
             */
            kThread::currentKT->ktvar->cv_flag = false;
            while (size == 0) {
                /*
                 * cv_flag can be true if the kThread were unblocked
                 * by another thread but by the time it acquires the
                 * mutex the queue is empty again. If so the kThread
                 * has been removed from the stack, thus put it back
                 * on the stack
                 */
                if (kThread::currentKT->ktvar->cv_flag) {
                    ktStack.push_back(*kThread::currentKT);
                }
                kThread::currentKT->ktvar->cv.wait(mlock);
            }
            /*
             * If cv_flag is not true, it means the kThread
             * experienced a spurious wake up and thus
             * has not been removed from the stack. Since
             * ktStack is intrusive the kThread can remove
             * itself from the stack.
             */
            if (!kThread::currentKT->ktvar->cv_flag) {
                ktStack.remove(*kThread::currentKT);
            }
        }
        ssize_t res = __removeMany(nqueue);
        /*
         * Each kThread unblocks the next kThread in case
         * PushMany was called. Only one kThread can always hold
         * the lock, so there is no benefit in unblocking all kThreads just
         * for them to be blocked by the mutex.
         * Chaining the unblocking has the benefit of distributing the cost
         * of multiple cv.notify_one() calls over producer + waiting consumers.
         */
        if (size != 0) __unBlock();

        return res;
    }

    /*
     * Push a single uThread to the queue and
     * wake one kThread up.
     */
    void __push(uThread *ut) {
        std::unique_lock<std::mutex> mlock(mtx, std::defer_lock);
        __spinLock(mlock);
        queue.push(*ut);
        size++;
        __unBlock();
    }

    /*
     * Push multiple uThreads to the queue, and wake one
     * kThread up. That kThread then wakes up another kThread
     * if there are still uThreads left in the queue.
     * uThreads are copied directly from the passed container
     * to the queue in bulk to avoid the overhead.
     */
    void __push(IntrusiveQueue <uThread> &utList, size_t count) {
        std::unique_lock<std::mutex> mlock(mtx, std::defer_lock);
        __spinLock(mlock);
        queue.transferAllFrom(utList);
        size += count;
        __unBlock();
    }

    /*
     * Whether the queue is empty or not.
     */
    bool __empty() const {
        return queue.empty();
    }

    /* ************** Scheduling wrappers *************/
    // Schedule a uThread on a cluster
    static void schedule(uThread *ut, const kThread &kt) {
        assert(ut != nullptr);
        kt.scheduler->__push(ut);
    }

    // Put uThread in the ready queue to be picked up by related kThreads
    void schedule(uThread *ut) {
        assert(ut != nullptr);
        // Scheduling uThread
        __push(ut);
    }

    // Schedule many uThreads
    void schedule(IntrusiveQueue <uThread> &queue, size_t count) {
        assert(!queue.empty());
        __push(queue, count);
    }

    uThread *nonBlockingSwitch(const kThread &kt) {
        IOHandler::iohandler.nonblockingPoll();
        uThread *ut = nullptr; /*  First check the local queue */
        IntrusiveQueue <uThread> &ktrq = kt.ktlocal->lrq;
        if (!ktrq.empty()) {   // If not empty, grab a uThread and run it
            ut = ktrq.front();
            ktrq.pop();
        } else {                // If empty try to fill

            ssize_t res = __tryPopMany(ktrq);   // Try to fill the local queue
            if (res > 0) {       // If there is more work start using it
                ut = ktrq.front();
                ktrq.pop();
            } else {        // If no work is available, Switch to defaultUt
                if (res == 0 && kt.currentUT->state == uThread::State::YIELD)
                    return nullptr; // if the running uThread yielded, continue running it
                ut = kt.mainUT;
            }
        }
        assert(ut != nullptr);
        return ut;
    }

    uThread *blockingSwitch(const kThread &kt) {
        /* before blocking inform the poller thread of our
         * intent.
         */
        IOHandler::iohandler.sem.post();

        ssize_t res = __popMany(kt.ktlocal->lrq);

        /*
         * if we signaled the poller thread, now it's the time
         * to signal it again that we are unblocked.
         */
        while (!IOHandler::iohandler.sem.trywait()) {}


        if (res == 0) return nullptr;
        // ktReadyQueue should not be empty at this point
        assert(!kt.ktlocal->lrq.empty());
        uThread *ut = kt.ktlocal->lrq.front();
        kt.ktlocal->lrq.pop();
        return ut;
    }

    /* assign a scheduler to a kThread */
    static Scheduler *getScheduler(const Cluster &cluster) {
        if (slowpath(cluster.scheduler == nullptr)) {
            std::unique_lock<std::mutex> ul(cluster.mtx);
            if (cluster.scheduler == nullptr)
                cluster.scheduler = new Scheduler();
        }
        return cluster.scheduler;
    }

    static void prepareBulkPush(uThread *ut) {
        ut->currentCluster->clustervar->bulkQueue.push(*ut);
        ut->currentCluster->clustervar->bulkCounter++;
    }

    static void bulkPush() {
        for (auto &x : Cluster::clusterList) {
            bulkPush(*x);
        }
    }

    static void bulkPush(const Cluster &cluster) {
        if (cluster.clustervar->bulkCounter > 0) {
            cluster.scheduler->schedule(cluster.clustervar->bulkQueue,
                                        cluster.clustervar->bulkCounter);
            cluster.clustervar->bulkCounter = 0;
        }
    }

    static void bulkPush(const kThread &kt) {
        // This only activates the cluster bulk push
        bulkPush(kt.localCluster);
    }
};
}  // namespace runtime
}  // namespace uThreads
#endif /* SRC_RUNTIME_SCHEDULER_01_H_ */
